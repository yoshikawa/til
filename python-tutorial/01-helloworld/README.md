Pythonを学ぶことで人工知能、機械学習、ビッグデータの解析などの科学分野で役に立っている。
FacebookやGoogleなどの企業で採用されている言語。

### 実行方法
#### スクリプトを実行する
ターミナルを起動して、`python hello_world.py`
```python
print('Hello World')
```
#### 対話モードで実行する
```
$ python
Python 3.6.4 (default, Feb  9 2018, 20:00:26)
[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> print('Hello World')
>>> quit()
```
対話を終了するには、`quit()またはexit()`で終了することが出来る。

### 文字列を出力する
「print」を用いると文字を出力（表示）することができる。
printの後ろの()の中に書いた文字が「コンソール」という画面に出力される。
文字列はシングルクォーテーション「'」またはダブルクォーテーション「"」で囲む必要がある。


### ソースコード内にコメントをしよう
行頭に「#」を書くことで、行末までコメントとみなされる。
コードに関するコメントをすることで、自分のみならず他者に対して

### ソースコードの文字コードの指定
[文字コードの話](https://docs.python.jp/3.6/howto/unicode.html)を参照すると、Python2とPython3ではエンコード方法がどうやら違うらしい。WindowsとMacでは文字コードも違うということを何処か片隅においておくことにする。
`# coding: -*- utf-8 -*-`
を冒頭に書くと、utf-8でエンコードしてくれる。日本語の文字化けをしないようにしている。

### クローリングとスクレイピング
#### クローラーとクローリング
「クローラー」とは自動的にWebページ上の情報を収集するためのプログラム。人間がブラウザでWebページ上の情報を収集するには膨大な時間を要する。そのため、短時間に収集するためにクローラーを用いる。クローラーで情報を収集することを「クローリング」という。
クローラーで一番身近なものがGoogle検索エンジン。検索エンジンはクローラーを使って世界中のWebページの情報を集め、蓄積。

#### スクレイピング
スクレイピングとは何か？クローラーは情報を収集する。スクレイピングはその収集した情報を解析して必要な情報を抜き出すことを指す。
「収集=>解析=>抽出=>保存=>出力」

### 注意点
クローリング・スクレイピングの際には情報の取扱には意識する必要がある。
- Webサイトの利用規約を確認して規約を守る
- robots.txt/robotsメタタグのアクセス制限内容を守る
- 制限されていない場合でもサーバーの不可を考えて適切なアクセス頻度にする
- rel="nofollow"が設定されているリンクはクローラーで辿らない
- クローラーする際にアクセス制限など情報収集を禁止する措置が取られていた場合は、即刻クロール処理を止め、すでに取得していた情報を含めて削除する

#### 収集したデータの取扱について
- 著作権上問題がある場合は個人での使用に留める
- 収集したデータをもとに検索サービスなどを提供する場合は、Webサイト・APIなどの利用規約を確認し、問題ない場合のみ提供する
- 利用規約が書かれていない場合でも勝手に公開するのではなく先方に確認を取る

---

### Wgetでクローラーを始める
#### Wgetとは
WgetはHTTP/FTPを使ってサーバーからファイルをダウンロードするためのオープンソースソフトウェア。Wgetはただ単にファイルをダウンロードするダウンローダの役割をするだけではなく、Webページを再帰的にダウンロードしたり、HTML内のリンクをローカルに変換できたりする。
特定の拡張子だけを指定してダウンロードすることや、ダウンロードの間隔を設定することもできる。

### Wgetのインストール
#### MacOSにインストール
Homebrewを利用する。
`brew install wget`
